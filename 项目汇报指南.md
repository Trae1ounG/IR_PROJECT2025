# 项目汇报完整指南

## 一、实现思路概述

### 1.1 任务分析

**核心任务**：在WebQ数据集上进行文档检索，返回top-K个相关passages

**评价指标**：
- Hit@1：第1个结果包含答案
- Hit@10：前10个结果包含答案
- Hit@100：前100个结果包含答案

**挑战**：
- 大规模语料库（75万+ Wikipedia passages）
- 需要高效检索
- 准确匹配答案

### 1.2 技术方案选择

#### 为什么选择双编码器（Bi-Encoder）？

| 方案 | 优势 | 劣势 | 适用场景 |
|------|------|------|----------|
| **双编码器** | • 可提前编码passages<br>• 检索速度快<br>• 适合大规模数据 | • query和passage独立编码，交互少 | ✅ 我们的场景 |
| 交叉编码器 | • query和passage深度交互<br>• 准确率高 | • 需要在线编码<br>• 检索慢 | 小规模精排 |
| BMF/TF-IDF | • 速度快<br>• 无需训练 | • 不理解语义<br>• 准确率低 | 基线对比 |

**结论**：双编码器最适合大规模检索场景

#### 为什么选择BERT？

- **预训练知识**：在海量文本上预训练，语义理解强
- **双向编码**：能捕捉上下文信息
- **[CLS] token**：天然提供序列级表示
- **成熟稳定**：社区支持好，易于实现

### 1.3 整体架构

```
训练阶段：
┌─────────────────────────────────────────────────────┐
│ Query + Positive Passage → BERT → Vector (q, p+)    │
│ Query + Negative Passage → BERT → Vector (q, p-)    │
│                                                      │
│ Contrastive Loss: 拉近(q, p+)，推远(q, p-)          │
└─────────────────────────────────────────────────────┘

检索阶段：
┌─────────────────────────────────────────────────────┐
│ 1. 预编码所有Passages → Passage Embeddings (离线)    │
│ 2. Query → BERT → Query Embedding (在线)            │
│ 3. 计算相似度: Similarity = Query·Passage            │
│ 4. 返回Top-K个Passages                              │
└─────────────────────────────────────────────────────┘
```

## 二、核心技术要点

### 2.1 双编码器模型

```python
class BiEncoder(nn.Module):
    def __init__(self):
        # 两个独立的BERT编码器
        self.question_encoder = BERT()
        self.passage_encoder = BERT()

    def forward(self, query, passage):
        # 分别编码
        q_vec = self.question_encoder(query)  # [batch, hidden]
        p_vec = self.passage_encoder(passage)  # [batch, hidden]
        return q_vec, p_vec
```

**关键设计**：
- Query和Passage独立编码
- 输出：[CLS] token的表示（768维向量）
- 使用L2归一化，方便计算余弦相似度

### 2.2 对比学习损失

**InfoNCE Loss**：
```
L = -log[exp(sim(q, p+)/τ) / Σexp(sim(q, p)/τ)]
```

**直观理解**：
- 正样本（p+）：query的正确答案passage
- 负样本（p-）：错误的passage
- 目标：让query与正样本相似度高，与负样本相似度低

**温度参数τ**：控制分布的锐度（我们设为0.05）

### 2.3 数据处理策略

**训练数据构造**：
1. 从WebQ训练集提取question和answers
2. 在Wikipedia语料库中查找包含答案的passages作为正样本
3. 随机选择其他passages作为负样本
4. 构造三元组：(question, positive_passage, negative_passage)

**数据规模**：
- 小规模测试：1000个三元组，约1800个unique passages
- 完整训练：10000个三元组，约15000-20000个unique passages

### 2.4 性能优化

**优化1：多GPU并行**
```python
os.environ['CUDA_VISIBLE_DEVICES'] = '6,7'
model = nn.DataParallel(model)
```
- 使用GPU 6和7
- Batch size从16增加到32
- 训练速度提升1.5-2倍

**优化2：预编码Passages**
```python
# 一次性编码所有passages并保存
passage_embeddings = encode_all_passages()
torch.save(passage_embeddings, 'embeddings.pt')

# 训练和测试时直接加载
passage_embeddings = torch.load('embeddings.pt')
```
- 编码时间：30-60分钟（只做一次）
- 后续验证：从30-60分钟降低到几秒
- **速度提升100-1000倍**

## 三、实现流程

### 3.1 数据预处理

**输入**：
- `webq-train.json`：训练数据（3718个问题）
- `webq-test.csv`：测试数据（2030个问题）
- `wiki_webq_corpus.tsv`：Wikipedia语料库（75万+文档）

**处理步骤**：
1. 加载语料库，建立passage索引
2. 处理训练数据：匹配答案在语料库中的位置
3. 构造训练三元组（question, pos_pid, neg_pid）
4. 保存为pickle和TSV格式

**输出**：
- `passages.pkl`：语料库字典
- `train_triples.tsv`：训练三元组
- `train_triples_qa.pkl`：问题-答案映射
- `test_questions.pkl` / `test_answers.pkl`：测试数据

### 3.2 Passages预编码

**目的**：提前编码所有passages，加速后续训练和测试

**流程**：
```python
# 1. 提取训练需要的passage IDs
required_pids = extract_from_train_triples()

# 2. 加载对应的passages
passages = {pid: corpus[pid] for pid in required_pids}

# 3. 批量编码
for batch in passages:
    embeddings = bert_encoder(batch)
    save(embeddings)

# 4. 保存到磁盘
torch.save({'embeddings': embeddings, 'ids': passage_ids})
```

**优化**：
- 批量处理（batch_size=128）
- GPU加速
- L2归一化
- 缓存到磁盘

### 3.3 模型训练

**训练循环**：
```python
for epoch in range(3):
    for batch in train_loader:
        # 1. 前向传播
        q_vec, pos_vec = model(question, pos_passage)
        _, neg_vec = model(question, neg_passage)

        # 2. 计算损失
        loss = contrastive_loss(q_vec, pos_vec, neg_vec)

        # 3. 反向传播
        loss.backward()
        optimizer.step()

    # 4. 验证
    hit_at_k = evaluate(model, val_data)
    if hit_at_k > best:
        save_model()
```

**超参数**：
- Learning Rate: 2e-5
- Batch Size: 32 (双GPU)
- Epochs: 3
- Optimizer: AdamW
- Scheduler: Linear Warmup

**验证策略**：
- 使用前100个测试样本作为验证集
- 每个epoch结束后评估
- 保存Hit@10最高的模型

### 3.4 测试评估

**测试流程**：
```python
# 1. 加载预编码的passage embeddings
passage_embeddings = torch.load('embeddings.pt')

# 2. 对每个测试query
for question, answers in test_set:
    # 编码query
    q_emb = model.encode(question)

    # 计算与所有passages的相似度
    scores = q_emb @ passage_embeddings.T

    # 获取top-100
    top_indices = torch.topk(scores, k=100)

    # 检查答案是否在top-K中
    for k in [1, 10, 100]:
        hit[k] = check_answer(top_indices[:k], answers)

# 3. 计算整体准确率
Hit@K = Σ hit[K] / N
```

**答案匹配**：
- 字符串子串匹配（不区分大小写）
- 只要top-K中任一passage包含任一答案即算命中

## 四、汇报要点

### 4.1 实现方案（2-3分钟）

**要点**：
1. 采用双编码器架构
2. 使用BERT预训练模型
3. 对比学习训练
4. 预编码优化

**话术**：
> "我们采用了双编码器架构进行检索。Query和Passage分别通过独立的BERT编码器编码为768维向量，通过计算向量相似度进行检索。训练时使用对比学习损失，拉近query与正样本passage的距离，推远与负样本的距离。为提升效率，我们实现了passage预编码机制，将验证和测试的速度提升了100倍以上。"

### 4.2 技术原理（3-4分钟）

**要点**：
1. BERT编码器原理
2. [CLS] token的作用
3. InfoNCE损失函数
4. Hit@K评价指标

**话术**：
> "BERT采用双向Transformer架构，能捕捉上下文信息。我们使用[CLS] token的输出作为整段文本的表示。损失函数采用InfoNCE，这是一种对比学习损失，通过温度参数τ控制分布的锐度。评价指标Hit@K衡量在前K个检索结果中找到答案的概率，这是信息检索中的标准指标。"

### 4.3 实验步骤（2-3分钟）

**关键点**：
1. 数据处理：从原始数据到训练三元组
2. 预编码：一次性编码所有passages
3. 模型训练：3个epoch，验证集选择最佳模型
4. 测试评估：在200个测试查询上评估

**数据**：
- 训练数据：1000个三元组（可扩展到10000）
- 验证数据：100个查询
- 测试数据：200个查询
- 语料库：75万+ Wikipedia passages

**话术**：
> "实验分为四个步骤。首先是数据预处理，我们从WebQ数据集提取问题和答案，在Wikipedia语料库中匹配包含答案的passages，构造正负样本对。然后是预编码阶段，我们将所有passages通过BERT编码并保存，这一步虽然耗时30分钟，但后续可以重复使用，大幅提升效率。第三步是模型训练，我们训练了3个epoch，在验证集上选择Hit@10最高的模型。最后在200个测试查询上评估，得到最终结果。"

### 4.4 结果汇报（1-2分钟）

**准备材料**：
1. `output/test_results/metrics.json`：整体指标
2. `output/test_results/hit_at_k.txt`：每个查询的结果
3. 可视化图表（可选）

**格式**：
```
Hit@1:   XX.XX%
Hit@10:  XX.XX%
Hit@100: XX.XX%
```

**话术**：
> "在200个测试查询上，我们的模型取得了以下结果：Hit@1达到XX%，Hit@10达到XX%，Hit@100达到XX%。这意味着在前10个检索结果中，有XX%的查询能找到包含答案的passage。相比基线方法，我们的方法提升了XX%（如果有对比）。"

### 4.5 创新点（加分项）

**可以提到的创新**：
1. **预编码优化**：将验证速度提升100倍
2. **多GPU并行**：利用DataParallel加速训练
3. **智能过滤**：只编码训练需要的passages，节省资源
4. **温度调优**：通过调整温度参数优化性能

## 五、常见问题准备

### Q1: 为什么选择双编码器而不是交叉编码器？

**答**：双编码器可以提前编码passages，检索时只需编码query，速度远快于交叉编码器。对于75万passages的大规模检索，双编码器是唯一可行的方案。

### Q2: 负样本如何选择？

**答**：我们使用随机负采样。从训练数据来看，WebQ数据集本身提供了negative_ctxs字段，我们优先使用这些经过筛选的负样本。如果没有，则随机选择不包含答案的passage。

### Q3: 如何避免在测试集上训练？

**答**：我们严格区分训练集、验证集和测试集。训练只用train_triples.tsv，验证使用前100个测试样本，测试在完整的200个测试查询上进行。验证集仅用于模型选择，不参与训练。

### Q4: Hit@K如何计算？

**答**：对于每个query，检索top-K个passages，检查这些passages的文本中是否包含答案（字符串子串匹配）。如果包含，Hit@K=1，否则=0。最终结果为所有query的平均值。

## 六、演示准备

### 6.1 代码演示（可选）

**重点展示**：
1. 数据加载和预处理
2. 模型定义（简洁版）
3. 预编码过程
4. 检索和评估

### 6.2 结果展示

**准备**：
1. 整体指标（表格形式）
2. 若干典型案例（成功/失败）
3. 与基线对比（如果有）

### 6.3 系统演示（可选）

**现场检索示例**：
```bash
python test_simple.py
# 展示某个query的top-10结果
# 检查是否包含答案
```

## 七、时间分配建议

| 部分 | 时间 | 重点 |
|------|------|------|
| 方案介绍 | 2-3分钟 | 架构图、技术选型 |
| 技术原理 | 3-4分钟 | BERT、损失函数、指标 |
| 实验步骤 | 2-3分钟 | 数据、训练、测试流程 |
| 结果汇报 | 1-2分钟 | 具体数字、分析 |
| 创新点 | 1分钟 | 优化策略 |
| Q&A | 5分钟 | 准备充分 |
| **总计** | **15-18分钟** | |

## 八、提交材料检查清单

- [ ] 源代码（所有.py文件）
- [ ] requirements.txt
- [ ] 运行脚本（run.sh或run_optimized.sh）
- [ ] 结果文件（hit_at_k.txt）✅ **必需**
- [ ] 详细结果（metrics.json, detailed_results.json）
- [ ] 实验报告PDF
- [ ] 一键运行说明

## 九、最后检查

### 代码运行
```bash
# 确保能一键运行
conda activate ir2025
bash run_optimized.sh
```

### 结果验证
```bash
# 检查结果文件
cat output/test_results/metrics.json
cat output/test_results/hit_at_k.txt
```

### 报告准备
- [ ] 填写实际数据
- [ ] 制作PPT（如需要）
- [ ] 准备演示
- [ ] 预演时间

---

**祝你汇报成功！🎉**
