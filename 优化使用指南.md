# 模型优化快速使用指南

## ✅ 已完成的优化

### 1. BGE-M3模型支持
- 支持最新的BGE-M3嵌入模型
- 自动检测并添加instruction前缀
- 支持E5、BGE等多种先进模型

### 2. 难负样本挖掘
- 为每个query构建难负样本池
- 训练时动态采样难负样本
- 改进的对比学习损失函数

### 3. 训练策略优化
- 优化的超参数配置
- 学习率warmup
- 梯度裁剪
- 早停机制

---

## 🚀 快速开始

### 训练模型（使用BGE-M3）

```bash
cd /ssd/tanyuqiao/IR_Project-main/data/IR_2025_Project
python train_simple.py
```

**训练配置**（已在代码中设置）:
- 模型: BAAI/bge-m3
- Batch Size: 16
- Epochs: 5
- 学习率: 2e-5
- 难负样本数: 2

### 测试模型

```bash
python test_simple.py
```

测试结果会保存在 `output/test_results/` 目录：
- `metrics.json` - 整体评估指标
- `hit_at_k.txt` - 每个查询的Hit@k结果（提交格式）
- `detailed_results.json` - 详细结果

---

## 🔧 切换不同模型

### 使用BERT-base（原版基线）

编辑 `train_simple.py` 和 `test_simple.py`：

```python
# 将第334行改为
MODEL_NAME = 'bert-base-uncased'
```

### 使用BGE-large（更大但可能更准确）

```python
MODEL_NAME = 'BAAI/bge-large-en-v1.5'
```

### 使用E5-large（微软模型）

```python
MODEL_NAME = 'intfloat/e5-large-v2'
```

**注意**：代码会自动检测是否需要添加instruction前缀，无需手动修改。

---

## 📊 预期性能提升

### 基线（BERT-base）
- Hit@1: ~30-35%
- Hit@10: ~50-55%
- Hit@100: ~65-70%

### 优化后（BGE-M3 + 难负样本）
- Hit@1: ~40-48% (**+10-13%**)
- Hit@10: ~60-68% (**+10-13%**)
- Hit@100: ~72-78% (**+7-8%**）

---

## 🎯 关键改进点

### 1. 难负样本挖掘
```python
# 自动为每个query构建难负样本池
# 训练时从池中随机采样2个难负样本
# 使用改进的对比学习损失
```

**效果**: 提升模型判别能力，特别是Hit@10指标

### 2. BGE-M3模型
```python
# 更强的预训练模型
# 支持512 token长度（BERT仅256）
# 使用query/passage前缀
```

**效果**: 整体性能提升，尤其是Hit@1

### 3. 训练优化
```python
# 10% warmup学习率调度
# 梯度裁剪防止爆炸
# 增加训练轮数到5个epoch
```

**效果**: 训练更稳定，收敛更好

---

## 📝 输出文件说明

### 训练输出
```
output/retriever/
└── best_model.pt          # 最佳模型权重
```

### 测试输出
```
output/test_results/
├── metrics.json           # 整体指标
├── hit_at_k.txt          # 提交格式结果
└── detailed_results.json # 详细结果
```

**提交格式** (`hit_at_k.txt`):
```
id	hit_at_1	hit_at_10	hit_at_100
0	1	1	1
1	0	1	1
2	1	1	1
...
```

---

## ⚙️ 高级配置

### 调整难负样本数量
```python
# 在train_simple.py第344行
NUM_HARD_NEGATIVES = 3  # 默认2，可调整为1-5
```

### 调整batch size
```python
# GPU内存不足时减小
BATCH_SIZE = 8  # 默认16

# GPU内存充足时增大
BATCH_SIZE = 32
```

### 调整学习率
```python
# BGE模型推荐
LR = 2e-5  # 默认

# 训练不稳定时
LR = 1e-5

# 收敛太慢时
LR = 3e-5
```

---

## 🐛 常见问题

### Q: 下载BGE-M3模型很慢怎么办？
**A**: 使用国内镜像
```bash
export HF_ENDPOINT=https://hf-mirror.com
python train_simple.py
```

### Q: CUDA内存不足（OOM）
**A**: 减小batch size和序列长度
```python
BATCH_SIZE = 8
MAX_LENGTH = 256
```

### Q: 训练需要多久？
**A**:
- BGE-M3: 约2-4小时/epoch（GPU）
- BERT-base: 约1-2小时/epoch
- 总计: BGE-M3约10-20小时（5 epochs）

### Q: 如何只测试不训练？
**A**: 直接运行test_simple.py即可
```bash
python test_simple.py
```

---

## 📚 技术细节

详见 `模型优化说明.md` 文档，包含：
- 完整技术原理
- 实现细节
- 性能分析
- 未来优化方向
- 参考论文

---

## 🎉 开始使用

```bash
# 1. 进入项目目录
cd /ssd/tanyuqiao/IR_Project-main/data/IR_2025_Project

# 2. 开始训练（会自动下载BGE-M3）
python train_simple.py

# 3. 训练完成后测试
python test_simple.py

# 4. 查看结果
cat output/test_results/metrics.json
```

祝训练顺利！🚀
