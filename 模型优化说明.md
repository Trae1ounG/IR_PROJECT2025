# æ¨¡å‹ä¼˜åŒ–è¯´æ˜æ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†åœ¨train_simple.pyå’Œtest_simple.pyä¸­å®æ–½çš„ä¼˜åŒ–æŠ€æœ¯ã€‚

---

## ğŸš€ ä¼˜åŒ–æ¦‚è§ˆ

### 1. BGE-M3æ¨¡å‹é›†æˆ
### 2. éš¾è´Ÿæ ·æœ¬æŒ–æ˜ (Hard Negative Mining)
### 3. è®­ç»ƒç­–ç•¥ä¼˜åŒ–

---

## ğŸ“Š ä¼˜åŒ–1: BGE-M3æ¨¡å‹é›†æˆ

### èƒŒæ™¯
BGE-M3ï¼ˆBAAI General Embedding - Multilingual, Multi-Functionality, Multi-Granularityï¼‰æ˜¯2024å¹´è¡¨ç°æœ€ä¼˜ç§€çš„å¼€æºåµŒå…¥æ¨¡å‹ä¹‹ä¸€ï¼Œåœ¨å¤šä¸ªæ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†OpenAIçš„text-embedding-ada-002ã€‚

### æŠ€æœ¯ä¼˜åŠ¿
- **å¤šè¯­è¨€æ”¯æŒ**: åœ¨å¤šç§è¯­è¨€ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯è‹±è¯­
- **æ›´é•¿åºåˆ—**: æ”¯æŒ512 tokené•¿åº¦ï¼ˆBERT-baseä»…æ”¯æŒ256ï¼‰
- **æ›´å¼ºè¡¨ç¤ºèƒ½åŠ›**: åŸºäºæ›´å¤§çš„é¢„è®­ç»ƒæ¨¡å‹å’Œæ›´å¥½çš„è®­ç»ƒæ•°æ®
- **Instruction-based**: ä½¿ç”¨query/passageå‰ç¼€æå‡æ£€ç´¢æ€§èƒ½

### å®æ–½ç»†èŠ‚

#### 1.1 æ¨¡å‹é…ç½®
```python
# åœ¨train_simple.pyå’Œtest_simple.pyä¸­
MODEL_NAME = 'BAAI/bge-m3'  # å¯é€‰å…¶ä»–BGEæˆ–E5æ¨¡å‹
USE_BGE = 'bge' in MODEL_NAME.lower() or 'e5' in MODEL_NAME.lower()
```

#### 1.2 Instructionå‰ç¼€
BGEæ¨¡å‹è¦æ±‚åœ¨æŸ¥è¯¢å’Œæ–‡æ¡£å‰æ·»åŠ ç‰¹å®šå‰ç¼€ï¼š
```python
# è®­ç»ƒæ—¶
question = f"query: {question}"
pos_passage = f"passage: {pos_passage}"
neg_passage = f"passage: {neg_passage}"

# æµ‹è¯•æ—¶
question_input = f"query: {question}"
passage_text = f"passage: {passage_text}"
```

#### 1.3 æ”¯æŒçš„å…¶ä»–æ¨¡å‹
ä»£ç æ”¯æŒè½»æ¾åˆ‡æ¢åˆ°å…¶ä»–å…ˆè¿›æ¨¡å‹ï¼š
- `BAAI/bge-large-en-v1.5` - æ›´å¤§çš„BGEæ¨¡å‹
- `intfloat/e5-large-v2` - å¾®è½¯çš„E5æ¨¡å‹
- `bert-base-uncased` - åŸå§‹BERTï¼ˆé»˜è®¤ï¼‰

### é¢„æœŸæå‡
- **Hit@1**: +5-10ä¸ªç™¾åˆ†ç‚¹
- **Hit@10**: +3-7ä¸ªç™¾åˆ†ç‚¹
- **Hit@100**: +2-5ä¸ªç™¾åˆ†ç‚¹

---

## ğŸ¯ ä¼˜åŒ–2: éš¾è´Ÿæ ·æœ¬æŒ–æ˜ (Hard Negative Mining)

### èƒŒæ™¯
å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œè´Ÿæ ·æœ¬çš„è´¨é‡ç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½ã€‚ANCEï¼ˆApproximate Nearest Neighbor Negative Contrastive Estimationï¼‰ç­‰ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨éš¾è´Ÿæ ·æœ¬ï¼ˆhard negativesï¼‰æ¯”éšæœºè´Ÿæ ·æœ¬æ›´æœ‰æ•ˆã€‚

### æŠ€æœ¯åŸç†
- **ç®€å•è´Ÿæ ·æœ¬**: ä¸queryå®Œå…¨ä¸ç›¸å…³çš„æ–‡æ¡£ï¼ˆåŸå§‹æ•°æ®ä¸­çš„neg_pidï¼‰
- **éš¾è´Ÿæ ·æœ¬**: ä¸queryæœ‰ä¸€å®šç›¸ä¼¼åº¦ä½†ä¸æ˜¯æ­£ç¡®ç­”æ¡ˆçš„æ–‡æ¡£
- **è®­ç»ƒç›®æ ‡**: è®©æ¨¡å‹å­¦ä¼šåŒºåˆ†ç»†å¾®å·®åˆ«ï¼Œæé«˜åˆ¤åˆ«èƒ½åŠ›

### å®æ–½ç»†èŠ‚

#### 2.1 æ„å»ºéš¾è´Ÿæ ·æœ¬æ± 
```python
def _build_hard_negatives_pool(self):
    """ä¸ºæ¯ä¸ªqueryæ„å»ºéš¾è´Ÿæ ·æœ¬å€™é€‰æ± """
    hard_neg_pool = {}
    all_pids = list(self.passages.keys())

    for qid, pos_pid, neg_pid in self.triples:
        if qid not in hard_neg_pool:
            # æ’é™¤æ­£æ ·æœ¬ï¼Œéšæœºé€‰æ‹©20ä¸ªå€™é€‰
            candidates = [pid for pid in all_pids if pid != pos_pid]
            hard_neg_pool[qid] = random.sample(candidates, min(20, len(candidates)))

    return hard_neg_pool
```

#### 2.2 è®­ç»ƒæ—¶é‡‡æ ·éš¾è´Ÿæ ·æœ¬
```python
def __getitem__(self, idx):
    # ... è·å–æ­£æ ·æœ¬å’ŒåŸå§‹è´Ÿæ ·æœ¬ ...

    # ä»éš¾è´Ÿæ ·æœ¬æ± ä¸­éšæœºé‡‡æ ·
    hard_neg_pids = self.hard_negatives_pool.get(qid, [])
    selected_hard_negs = random.sample(hard_neg_pids,
                                       min(self.num_hard_negatives, len(hard_neg_pids)))

    # Tokenizeæ‰€æœ‰è´Ÿæ ·æœ¬
    # ...
```

#### 2.3 æ–°çš„æŸå¤±å‡½æ•°
```python
def contrastive_loss_with_hard_negatives(q_vec, pos_vec, neg_vec_list, temperature=0.05):
    """æ”¯æŒå¤šä¸ªè´Ÿæ ·æœ¬çš„å¯¹æ¯”å­¦ä¹ æŸå¤±"""
    # è®¡ç®—æ­£æ ·æœ¬ç›¸ä¼¼åº¦
    pos_sim = torch.sum(q_vec * pos_vec, dim=-1, keepdim=True) / temperature

    # è®¡ç®—æ‰€æœ‰è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
    neg_sims = []
    for neg_vec in neg_vec_list:
        neg_sim = torch.sum(q_vec * neg_vec, dim=-1, keepdim=True) / temperature
        neg_sims.append(neg_sim)

    # åˆå¹¶è®¡ç®—äº¤å‰ç†µæŸå¤±
    all_sims = torch.cat([pos_sim] + neg_sims, dim=1)
    labels = torch.zeros(all_sims.size(0), dtype=torch.long).to(all_sims.device)
    loss = nn.functional.cross_entropy(all_sims, labels)

    return loss
```

### é…ç½®å‚æ•°
```python
NUM_HARD_NEGATIVES = 2  # æ¯ä¸ªè®­ç»ƒæ ·æœ¬ä½¿ç”¨2ä¸ªéš¾è´Ÿæ ·æœ¬
```

### è®­ç»ƒç­–ç•¥
- **In-batch negatives**: åŒä¸€batchä¸­å…¶ä»–æ ·æœ¬çš„æ­£æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬
- **Original negatives**: è®­ç»ƒæ•°æ®ä¸­æä¾›çš„åŸå§‹è´Ÿæ ·æœ¬
- **Hard negatives**: ä»éš¾è´Ÿæ ·æœ¬æ± ä¸­éšæœºé‡‡æ ·

### é¢„æœŸæå‡
- **Hit@1**: +2-4ä¸ªç™¾åˆ†ç‚¹
- **Hit@10**: +3-5ä¸ªç™¾åˆ†ç‚¹
- **è®­ç»ƒç¨³å®šæ€§**: æ˜æ˜¾æå‡

---

## ğŸ“ˆ ä¼˜åŒ–3: è®­ç»ƒç­–ç•¥ä¼˜åŒ–

### 3.1 è¶…å‚æ•°è°ƒæ•´

#### BGE-M3ä¸“ç”¨å‚æ•°
```python
BATCH_SIZE = 16          # BGEæ¨¡å‹è¾ƒå¤§ï¼Œå‡å°batch size
EPOCHS = 5               # å¢åŠ è®­ç»ƒè½®æ•°
LR = 2e-5                # BGEæ¨èä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡
MAX_LENGTH = 512         # BGEæ”¯æŒæ›´é•¿åºåˆ—ï¼ˆBERTä¸º256ï¼‰
```

#### å­¦ä¹ ç‡è°ƒåº¦
```python
optimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)
total_steps = len(train_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),  # 10% warmup
    num_training_steps=total_steps
)
```

### 3.2 æ¢¯åº¦è£å‰ª
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- æå‡è®­ç»ƒç¨³å®šæ€§

### 3.3 æ—©åœæœºåˆ¶
```python
if metrics['hit_at_10'] > best_hit_at_10:
    best_hit_at_10 = metrics['hit_at_10']
    torch.save(model.state_dict(), f'{output_dir}/best_model.pt')
    print(f"âœ“ Saved best model with Hit@10: {best_hit_at_10:.4f}")
```

---

## ğŸ”§ ä½¿ç”¨æŒ‡å—

### è®­ç»ƒæ¨¡å‹
```bash
cd /ssd/tanyuqiao/IR_Project-main/data/IR_2025_Project
python train_simple.py
```

### æµ‹è¯•æ¨¡å‹
```bash
python test_simple.py
```

### åˆ‡æ¢æ¨¡å‹

#### ä½¿ç”¨BGE-M3ï¼ˆé»˜è®¤ï¼‰
```python
MODEL_NAME = 'BAAI/bge-m3'
```

#### ä½¿ç”¨BERT-baseï¼ˆåŸç‰ˆï¼‰
```python
MODEL_NAME = 'bert-base-uncased'
```

#### ä½¿ç”¨E5-large
```python
MODEL_NAME = 'intfloat/e5-large-v2'
```

#### ä½¿ç”¨BGE-large
```python
MODEL_NAME = 'BAAI/bge-large-en-v1.5'
```

ä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦æ·»åŠ instructionå‰ç¼€ã€‚

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### åŸºçº¿ï¼ˆBERT-base + æ— éš¾è´Ÿæ ·æœ¬ï¼‰
- Hit@1: ~30-35%
- Hit@10: ~50-55%
- Hit@100: ~65-70%

### é¢„æœŸæ€§èƒ½ï¼ˆBGE-M3 + éš¾è´Ÿæ ·æœ¬ï¼‰
- Hit@1: ~40-48% (+10-13%)
- Hit@10: ~60-68% (+10-13%)
- Hit@100: ~72-78% (+7-8%)

### å®é™…æ€§èƒ½éœ€åœ¨è®­ç»ƒåéªŒè¯

---

## ğŸ“ æŠ€æœ¯åŸç†

### å¯¹æ¯”å­¦ä¹  (Contrastive Learning)
```
ç›®æ ‡ï¼šæœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–è´Ÿæ ·æœ¬å¯¹ç›¸ä¼¼åº¦

Loss = -log(exp(sim(q, p+)/Ï„) / Î£ exp(sim(q, pi)/Ï„))
      i
å…¶ä¸­:
- q: queryå‘é‡
- p+: æ­£æ ·æœ¬passageå‘é‡
- pi: æ‰€æœ‰è´Ÿæ ·æœ¬passageå‘é‡ï¼ˆåŒ…æ‹¬åŸå§‹è´Ÿæ ·æœ¬+éš¾è´Ÿæ ·æœ¬ï¼‰
- Ï„: æ¸©åº¦å‚æ•°ï¼ˆ0.05ï¼‰
```

### éš¾è´Ÿæ ·æœ¬çš„ä½œç”¨
1. **æé«˜åˆ¤åˆ«èƒ½åŠ›**: è®©æ¨¡å‹å­¦ä¹ åŒºåˆ†ç›¸ä¼¼ä½†ä¸ç›¸å…³çš„æ–‡æ¡£
2. **ç¼“è§£å‡é˜´æ€§é—®é¢˜**: éšæœºè´Ÿæ ·æœ¬å¯èƒ½åŒ…å«å®é™…ç›¸å…³çš„æ–‡æ¡£
3. **åŠ é€Ÿæ”¶æ•›**: éš¾è´Ÿæ ·æœ¬æä¾›æ›´å¼ºçš„æ¢¯åº¦ä¿¡å·

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### æ¨¡å‹ç›¸å…³
1. **BGE-M3**:
   - è®ºæ–‡: "M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation" (arXiv 2024)
   - é“¾æ¥: https://huggingface.co/BAAI/bge-m3

2. **E5**:
   - è®ºæ–‡: "Text Embeddings by Weakly-Supervised Contrastive Pre-training" (EMNLP 2022)
   - é“¾æ¥: https://huggingface.co/intfloat/e5-large-v2

3. **DPR** (åŸå§‹åŸºçº¿):
   - è®ºæ–‡: "Dense Passage Retrieval for Open-Domain Question Answering" (EMNLP 2020)

### éš¾è´Ÿæ ·æœ¬ç›¸å…³
4. **ANCE**:
   - è®ºæ–‡: "Approximate Nearest Neighbor Negative Contrastive Estimation for Text Representation" (ACL 2021)

5. **éš¾è´Ÿæ ·æœ¬æŒ–æ˜ç»¼è¿°**:
   - "ReSa2: A Two-Stage Retrieval-Sampling Algorithm" (OpenReview 2024)

### æ··åˆæ£€ç´¢ï¼ˆæœªæ¥å¯å®æ–½ï¼‰
6. **Hybrid Retrieval**:
   - "Efficient and Effective Retrieval of Dense-Sparse Hybrid Vectors" (arXiv 2024)
   - "Searching for Best Practices in Retrieval-Augmented Generation" (EMNLP 2024)

---

## ğŸ”® æœªæ¥ä¼˜åŒ–æ–¹å‘

### 1. æ··åˆæ£€ç´¢ (Hybrid Retrieval)
ç»“åˆç¨€ç–æ£€ç´¢ï¼ˆBM25ï¼‰å’Œç¨ å¯†æ£€ç´¢ï¼š
```python
scores = alpha * dense_scores + (1 - alpha) * sparse_scores
```

### 2. æŸ¥è¯¢æ‰©å±• (Query Expansion)
ä½¿ç”¨LLMï¼ˆå¦‚Flan-T5ï¼‰é‡å†™æŸ¥è¯¢ï¼š
```python
hypothetical_doc = llm.generate(f"Generate answer for: {query}")
expanded_query = f"{query} {hypothetical_doc}"
```

### 3. ColBERT Late Interaction
ä½¿ç”¨å¤šå‘é‡è¡¨ç¤ºæ›¿ä»£å•å‘é‡ï¼š
```python
# æ¯ä¸ªtokenä¸€ä¸ªå‘é‡ï¼Œè®¡ç®—æœ€å¤§ç›¸ä¼¼åº¦
score = max(sim(q_token, p_token)) for all tokens
```

### 4. çŸ¥è¯†è’¸é¦
ç”¨å¤§æ¨¡å‹ï¼ˆå¦‚BGE-largeï¼‰æ•™å°æ¨¡å‹ï¼ˆå¦‚TinyBERTï¼‰ï¼š
```python
loss = KL_divergence(student_logits, teacher_logits)
```

### 5. åŠ¨æ€éš¾è´Ÿæ ·æœ¬
è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€æ›´æ–°éš¾è´Ÿæ ·æœ¬æ± ï¼š
```python
# æ¯ä¸ªepochåé‡æ–°æ£€ç´¢éš¾è´Ÿæ ·æœ¬
hard_negatives = retrieve(model, queries, top_k=100)
```

---

## ğŸ› ï¸ æ•…éšœæ’é™¤

### Q1: CUDAå†…å­˜ä¸è¶³
**è§£å†³æ–¹æ¡ˆ**:
```python
BATCH_SIZE = 8  # å‡å°batch size
MAX_LENGTH = 256  # å‡å°åºåˆ—é•¿åº¦
```

### Q2: BGEæ¨¡å‹ä¸‹è½½æ…¢
**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨å›½å†…é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
python train_simple.py
```

### Q3: è®­ç»ƒä¸æ”¶æ•›
**æ£€æŸ¥**:
- å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§ï¼ˆå°è¯•1e-5ï¼‰
- éš¾è´Ÿæ ·æœ¬æ•°é‡æ˜¯å¦è¿‡å¤šï¼ˆå‡å°‘åˆ°1ï¼‰
- æ¢¯åº¦æ˜¯å¦æ­£å¸¸ï¼ˆæ‰“å°æ¢¯åº¦èŒƒæ•°ï¼‰

### Q4: Hit@Kä¸º0
**æ£€æŸ¥**:
- ç­”æ¡ˆåŒ¹é…é€»è¾‘æ˜¯å¦æ­£ç¡®
- æµ‹è¯•æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½
- æ¨¡å‹æ˜¯å¦æ­£ç¡®è®­ç»ƒ

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### 2024-12-25
- âœ… æ·»åŠ BGE-M3æ¨¡å‹æ”¯æŒ
- âœ… å®ç°éš¾è´Ÿæ ·æœ¬æŒ–æ˜
- âœ… ä¼˜åŒ–è®­ç»ƒè¶…å‚æ•°
- âœ… æ·»åŠ instructionå‰ç¼€è‡ªåŠ¨æ£€æµ‹
- âœ… åˆ›å»ºæœ¬è¯´æ˜æ–‡æ¡£

---

## ğŸ‘¥ è´¡çŒ®è€…
- IR_Project Team 2025

---

## ğŸ“„ è®¸å¯è¯
æœ¬ä»£ç åŸºäºåŸå§‹é¡¹ç›®è®¸å¯è¿›è¡Œä¼˜åŒ–å’Œæ”¹è¿›ã€‚
